{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from stix2.v21 import (ThreatActor, Identity, AttackPattern, Campaign, IntrusionSet, Relationship, ExternalReference, Bundle, Grouping)\n",
    "from efficient_apriori import apriori\n",
    "import re\n",
    "from pyattck import Attck\n",
    "import requests\n",
    "import json\n",
    "from stix2 import MemoryStore, Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings Values - for Apriori Algorithm\n",
    "# TODO - 2: Do we need to adjust these numbers, in order to refine our algorithm metrics??\n",
    "confidenceLevel = 0.70\n",
    "supportLevel = 0.05\n",
    "abstract = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G0016'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NameToCode(gName):\n",
    "    # create an instance of the Attck class\n",
    "    attck = Attck()\n",
    "\n",
    "    # get all APT groups in the framework\n",
    "    apt_groups = attck.enterprise.actors\n",
    "\n",
    "    # create a dictionary mapping APT group names to G codes\n",
    "    group_to_gcode = {}\n",
    "    for group in apt_groups:\n",
    "        if group.name == gName:\n",
    "            gcode = group.id\n",
    "            return gcode\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTestSet():\n",
    "    df = pd.read_csv(\"datasets/Categorized_Adversary_TTPs.csv\").loc[:, ['mitre_attack_name', 'mitre_attack_ttps']] # sample dataset of attacks\n",
    "    test_threat_profiles = {}\n",
    "    for row in df.values:\n",
    "        gcode = NameToCode(row[0].strip())\n",
    "        tcodes = (row[1].strip(\"'][\").replace(\"'\",\"\").split(', '))\n",
    "        test_threat_profiles[gcode] = tcodes\n",
    "    \n",
    "    return test_threat_profiles\n",
    "    \n",
    "def GenerateAprioriLists():\n",
    "\n",
    "    #Data from Scott at Tidal Cyber\n",
    "    ttpLists = []\n",
    "    df1 = json.load(open(\"datasets/TidalCyberData/otx_running.json\")) [1:]\n",
    "    for row in df1:\n",
    "        if len(row[7]) > 0 and type(row[7]) == type([]):\n",
    "            ttpLists.append([\"'\" + ttp + \"'\" for ttp in row[7]])\n",
    "    \n",
    "    df2 = json.load(open(\"datasets/TidalCyberData/Tidal OSINT Technique Extraction.json\"))\n",
    "    for row in df2:\n",
    "        if len(row['Tidal Extracted Techniques']) > 0:\n",
    "            ttpLists.append([\"'\" + ttp2 + \"'\" for ttp2 in row['Tidal Extracted Techniques']])\n",
    "    # Some data comes from this dataset with TTPs\n",
    "    #df3 = pd.read_csv(\"datasets/Categorized_Adversary_TTPs.csv\") # sample dataset of attacks\n",
    "\n",
    "    # More data is gained by using attck data from tool and malware TTPs\n",
    "    attack = Attck()\n",
    "    \n",
    "    malwares = attack.enterprise.malwares + attack.enterprise.tools\n",
    "    \n",
    "    for malware in malwares:\n",
    "        ttpLists.append([\"'\" + ttp.id + \"'\" for ttp in malware.techniques])\n",
    "\n",
    "    # To use the apriori we need to generate a list of lists\n",
    "    # aprList = ttpLists\n",
    "    # for row in df3.values:\n",
    "    #     aprList.append((row[13].strip('][').split(', ')))\n",
    "\n",
    "    # get rid of empty sets\n",
    "    return [ttp for ttp in aprList if len(ttp) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AbstractTTPs(ttpList):\n",
    "    # Take sub-techniques and remove the .### to abstract them to parent techniques \n",
    "    for i in range(0,len(ttpList)):\n",
    "        ttpList[i] = [re.sub(r'\\.[0-9]+', '', ttp) for ttp in ttpList[i]]\n",
    "    return ttpList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a list of lists and returns a list of rules sorted by size \n",
    "def AprioriMining(aprList):\n",
    "    # Perform apriori rule association mining\n",
    "    itemsets, rules = apriori(aprList, min_support=supportLevel, min_confidence=confidenceLevel)\n",
    "    \n",
    "    # Sort by size to get the 1:1 mappings first and so on. \n",
    "    ruleNums = np.array([len(rule.lhs+rule.rhs) for rule in rules])\n",
    "    rules = np.array(rules)\n",
    "    inds = ruleNums.argsort()[::]\n",
    "    rules = rules[inds]\n",
    "    \n",
    "    # Maximum rule size of 4 to limit number of rules, any rules with size > 4 are redundant anyways\n",
    "    rules = [x for x in filter(lambda rule: len(rule.lhs+rule.rhs) <= 4, rules)]\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads latest MITRE framework from the branch\n",
    "def get_data_from_branch(domain):\n",
    "    \"\"\"get the ATT&CK STIX data from MITRE/CTI. Domain should be 'enterprise-attack', 'mobile-attack' or 'ics-attack'. Branch should typically be master.\"\"\"\n",
    "    stix_json = requests.get(f\"https://raw.githubusercontent.com/mitre-attack/attack-stix-data/master/{domain}/{domain}.json\").json()\n",
    "    return MemoryStore(stix_data=stix_json[\"objects\"])\n",
    "\n",
    "src = get_data_from_branch(\"enterprise-attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TTPs from seed, where seed is an APT group code: GXXXX\n",
    "def ExtractSeedTTPs(seed):\n",
    "    seeds = []\n",
    "    # This runs but says that \"actors\" is not found\n",
    "    actor = actors[seed]\n",
    "    for ttp in actor.techniques:\n",
    "        seeds.append(ttp.id)\n",
    "        \n",
    "    # If we are using abstracted TTPs then remove sub technique\n",
    "    if (abstract):\n",
    "        for i in range(0, len(seeds)):\n",
    "            seeds[i] = re.sub(r'\\.[0-9]+', '', seeds[i])\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExportBundle(bundle, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(bundle.serialize())\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each rule has a confidence, lift, support, lhs, rhs, conviction, rule power factor (RPF), \n",
    "\n",
    "# Takes an APTGroup and generates hypothesized relationships and attack pattern objects\n",
    "# Returns a tuple of: (profile, attackPatterns, relationships, attackPatterns)\n",
    "def CreateRelationships(seeds, aprioriLists):\n",
    "    profile = {} #stores the TTP and tactic for easy logging\n",
    "    seen = [] #stores TTPs that have been seen by the algorithm\n",
    "    attackPatterns = {} #stores generated attack pattern objects\n",
    "    relationships = {} # stores generated relationship objects\n",
    "    # attackPatterns = {} # stores generated grouping objects\n",
    "\n",
    "    # Get seeds out of the APT group\n",
    "    # seeds = ExtractSeedTTPs(APTGroup)\n",
    "    # seeds = [\"T1566\", \"T1204\"]\n",
    "    \n",
    "    # Perform Association Rule Mining\n",
    "    rules = AprioriMining(aprioriLists) \n",
    "    \n",
    "    # Add seeds to activity-attack-graph as nodes\n",
    "    for seed in seeds:\n",
    "       ## print(seed)\n",
    "        profile[seed] = 1\n",
    "        # Get the stix object for the TTP\n",
    "        ttp = src.query([ Filter(\"external_references.external_id\", \"=\", seed) ])[0]\n",
    "        \n",
    "        # Create and add objects to lists, context is used to identify seeds from hypothesized events\n",
    "        attackPatterns[seed] = src.query([ Filter(\"external_references.external_id\", \"=\", seed) ])[0]\n",
    "    \n",
    "    # print(profile)\n",
    "    \n",
    "    seedTotals = [\"'\"+x+\"'\" for x in seeds]\n",
    "    # Use a queue to iterate through and create a tree of TTPs\n",
    "    while len(seeds) > 0:\n",
    "        for rule in rules:\n",
    "            # Check to see if the left hand side of a rule is satisfied \n",
    "            if \"'\"+seeds[0]+\"'\" in rule.lhs and set(rule.lhs).issubset(seedTotals):\n",
    "                # If the lhs is satisfied, then loop through each TTP in the rhs \n",
    "                for ttpName in rule.rhs:\n",
    "                    # If this TTP hasn't been visited already then create STIX objects\n",
    "                    if ttpName not in seen:\n",
    "                        ttp = src.query([ Filter(\"external_references.external_id\", \"=\", ttpName[1:-1]) ])[0]\n",
    "                        \n",
    "                        # rule tactics \n",
    "                        tactics = []\n",
    "                        for i in ttp['kill_chain_phases']:\n",
    "                            tactics.append(i['phase_name'])\n",
    "                        \n",
    "                        # create a new grouping object with the rule name. \n",
    "                        \n",
    "                        attackPatterns[ttpName[1:-1]] = ttp\n",
    "                        seedTotals.append(ttpName)\n",
    "                        seen.append(ttpName)\n",
    "                        seeds.append(ttp['external_references'][0]['external_id'])\n",
    "\n",
    "                    # if the relationship already exists between two objects then we take the one with higher confidence\n",
    "                    # make sure the exact relationship does not alraedy exist\n",
    "                    if not (attackPatterns[seeds[0]]['id'], attackPatterns[ttpName[1:-1]]['id']) in relationships:\n",
    "                        # if the opposite relationship exists then choose the one with the highest confidence to add\n",
    "                        # we do this because if we do not then on the graph there will be two arrows going opposite directions to connect the same 2 TTPs\n",
    "                        # this causes clutter and makes the confidence values unreadable since they will be layered on top of each other\n",
    "                        if (attackPatterns[ttpName[1:-1]]['id'], attackPatterns[seeds[0]]['id']) in relationships:\n",
    "                            # if existing relationship has higher confidence, move on\n",
    "                            if float(relationships[(attackPatterns[ttpName[1:-1]]['id'], attackPatterns[seeds[0]]['id'])]['relationship_type']) > rule.confidence:\n",
    "                                continue # move to next iteration of for loop, so code after this statement won't execute\n",
    "                            # if existing relationship has lower confidence, delete it and let the new relationship take it's place\n",
    "                            else:\n",
    "                                del relationships[(attackPatterns[ttpName[1:-1]]['id'], attackPatterns[seeds[0]]['id'])]\n",
    "                        relationships[(attackPatterns[seeds[0]]['id'], attackPatterns[ttpName[1:-1]]['id'])] = Relationship(attackPatterns[seeds[0]]['id'], str(round(rule.confidence, 3)), attackPatterns[ttpName[1:-1]]['id'])\n",
    "                        if not((ttpName[1:-1] in profile.keys() and profile[ttpName[1:-1]] >= 1)):\n",
    "                            profile[ttpName[1:-1]] = round(rule.confidence, 3)\n",
    " \n",
    "        # pop to progress the queue\n",
    "        seeds.pop(0) \n",
    "    \n",
    "    for seed in seeds:\n",
    "        profile[seed] = 1\n",
    "        \n",
    "\n",
    "    return (profile, attackPatterns, relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "aprioriList = AbstractTTPs(GenerateAprioriLists()) # generate lists for apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "intelSeed = [\"T1566\", \"T1204\"] # Use-case: Observed TTPs\n",
    "profile, attackPatterns, relationships = CreateRelationships(intelSeed, aprioriList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('attack-pattern--7385dfaf-6886-4229-9ecd-6fd678040830', 'attack-pattern--8c32eb4d-805f-4fc5-bf60-c4d476c131b5'): Relationship(type='relationship', spec_version='2.1', id='relationship--0938da37-9b6a-4ace-9ab1-90793415914b', created='2023-04-01T00:58:46.645Z', modified='2023-04-01T00:58:46.645Z', relationship_type='0.77', source_ref='attack-pattern--7385dfaf-6886-4229-9ecd-6fd678040830', target_ref='attack-pattern--8c32eb4d-805f-4fc5-bf60-c4d476c131b5', revoked=False), ('attack-pattern--b3d682b6-98f2-4fb0-aa3b-b4df007ca70a', 'attack-pattern--8c32eb4d-805f-4fc5-bf60-c4d476c131b5'): Relationship(type='relationship', spec_version='2.1', id='relationship--ba591998-4098-42bf-9a21-9b533074bbe7', created='2023-04-01T00:58:46.649001Z', modified='2023-04-01T00:58:46.649001Z', relationship_type='0.794', source_ref='attack-pattern--b3d682b6-98f2-4fb0-aa3b-b4df007ca70a', target_ref='attack-pattern--8c32eb4d-805f-4fc5-bf60-c4d476c131b5', revoked=False), ('attack-pattern--b3d682b6-98f2-4fb0-aa3b-b4df007ca70a', 'attack-pattern--7385dfaf-6886-4229-9ecd-6fd678040830'): Relationship(type='relationship', spec_version='2.1', id='relationship--2159478d-8a29-49d8-9515-a37e8eabe9c9', created='2023-04-01T00:58:46.650184Z', modified='2023-04-01T00:58:46.650184Z', relationship_type='0.817', source_ref='attack-pattern--b3d682b6-98f2-4fb0-aa3b-b4df007ca70a', target_ref='attack-pattern--7385dfaf-6886-4229-9ecd-6fd678040830', revoked=False), ('attack-pattern--a62a8db3-f23a-4d8f-afd6-9dbc77e7813b', 'attack-pattern--7385dfaf-6886-4229-9ecd-6fd678040830'): Relationship(type='relationship', spec_version='2.1', id='relationship--21db9b25-e914-4608-81a8-eb459287134c', created='2023-04-01T00:58:46.653Z', modified='2023-04-01T00:58:46.653Z', relationship_type='0.856', source_ref='attack-pattern--a62a8db3-f23a-4d8f-afd6-9dbc77e7813b', target_ref='attack-pattern--7385dfaf-6886-4229-9ecd-6fd678040830', revoked=False), ('attack-pattern--a62a8db3-f23a-4d8f-afd6-9dbc77e7813b', 'attack-pattern--8c32eb4d-805f-4fc5-bf60-c4d476c131b5'): Relationship(type='relationship', spec_version='2.1', id='relationship--b0a4f15c-9078-44c4-8cdb-c832f0e35c55', created='2023-04-01T00:58:46.653944Z', modified='2023-04-01T00:58:46.653944Z', relationship_type='0.904', source_ref='attack-pattern--a62a8db3-f23a-4d8f-afd6-9dbc77e7813b', target_ref='attack-pattern--8c32eb4d-805f-4fc5-bf60-c4d476c131b5', revoked=False), ('attack-pattern--a62a8db3-f23a-4d8f-afd6-9dbc77e7813b', 'attack-pattern--b3d682b6-98f2-4fb0-aa3b-b4df007ca70a'): Relationship(type='relationship', spec_version='2.1', id='relationship--45f007e3-42aa-4457-9445-38b1207d0e49', created='2023-04-01T00:58:46.653944Z', modified='2023-04-01T00:58:46.653944Z', relationship_type='0.726', source_ref='attack-pattern--a62a8db3-f23a-4d8f-afd6-9dbc77e7813b', target_ref='attack-pattern--b3d682b6-98f2-4fb0-aa3b-b4df007ca70a', revoked=False), ('attack-pattern--355be19c-ffc9-46d5-8d50-d6a036c675b6', 'attack-pattern--7385dfaf-6886-4229-9ecd-6fd678040830'): Relationship(type='relationship', spec_version='2.1', id='relationship--d7106577-637c-4f9b-a35a-de3b275484d8', created='2023-04-01T00:58:46.655998Z', modified='2023-04-01T00:58:46.655998Z', relationship_type='0.856', source_ref='attack-pattern--355be19c-ffc9-46d5-8d50-d6a036c675b6', target_ref='attack-pattern--7385dfaf-6886-4229-9ecd-6fd678040830', revoked=False), ('attack-pattern--355be19c-ffc9-46d5-8d50-d6a036c675b6', 'attack-pattern--8c32eb4d-805f-4fc5-bf60-c4d476c131b5'): Relationship(type='relationship', spec_version='2.1', id='relationship--417d2487-8d9e-4f01-8fef-d016cf64ba6f', created='2023-04-01T00:58:46.656992Z', modified='2023-04-01T00:58:46.656992Z', relationship_type='0.904', source_ref='attack-pattern--355be19c-ffc9-46d5-8d50-d6a036c675b6', target_ref='attack-pattern--8c32eb4d-805f-4fc5-bf60-c4d476c131b5', revoked=False), ('attack-pattern--355be19c-ffc9-46d5-8d50-d6a036c675b6', 'attack-pattern--b3d682b6-98f2-4fb0-aa3b-b4df007ca70a'): Relationship(type='relationship', spec_version='2.1', id='relationship--4fd69786-55ec-41ef-9e00-fb2ee2fa749d', created='2023-04-01T00:58:46.656992Z', modified='2023-04-01T00:58:46.656992Z', relationship_type='0.726', source_ref='attack-pattern--355be19c-ffc9-46d5-8d50-d6a036c675b6', target_ref='attack-pattern--b3d682b6-98f2-4fb0-aa3b-b4df007ca70a', revoked=False), ('attack-pattern--355be19c-ffc9-46d5-8d50-d6a036c675b6', 'attack-pattern--a62a8db3-f23a-4d8f-afd6-9dbc77e7813b'): Relationship(type='relationship', spec_version='2.1', id='relationship--4013faaf-6955-4d5e-9381-95186e62adee', created='2023-04-01T00:58:46.657944Z', modified='2023-04-01T00:58:46.657944Z', relationship_type='0.706', source_ref='attack-pattern--355be19c-ffc9-46d5-8d50-d6a036c675b6', target_ref='attack-pattern--a62a8db3-f23a-4d8f-afd6-9dbc77e7813b', revoked=False)}\n"
     ]
    }
   ],
   "source": [
    "# Displays the results of the Apiori Algorithm \n",
    "print(relationships)\n",
    "\n",
    "# This is the hypothesized profile\n",
    "apt_x = profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle STIX Objects for Visualization\n",
    "bundle = Bundle(list(attackPatterns.values())+list(relationships.values()), allow_custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export bundle for visualization here: https://github.com/yukh1402/cti-stix-diamond-activity-attack-graph\n",
    "ExportBundle(bundle, \"test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'T1114': 1, 'T1078': 1, 'T1566': 1, 'T1056': 1, 'T1204': 1, 'T1059': 1, 'T1564': 1, 'T1071': 1, 'T1090': 1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m apt, t_codes \u001b[38;5;129;01min\u001b[39;00m threat_profiles\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     21\u001b[0m     ttp_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(t_codes\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m---> 22\u001b[0m     profile, attackPatterns, relationships, groupings \u001b[38;5;241m=\u001b[39m \u001b[43mCreateRelationships\u001b[49m\u001b[43m(\u001b[49m\u001b[43mttp_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maprioriList\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     extended_threat_profiles[apt] \u001b[38;5;241m=\u001b[39m profile\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(extended_threat_profiles[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG0085\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[31], line 23\u001b[0m, in \u001b[0;36mCreateRelationships\u001b[1;34m(seeds, aprioriLists)\u001b[0m\n\u001b[0;32m     21\u001b[0m profile[seed] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Get the stix object for the TTP\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m ttp \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[43mFilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexternal_references.external_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create and add objects to lists, context is used to identify seeds from hypothesized events\u001b[39;00m\n\u001b[0;32m     26\u001b[0m groupings[seed] \u001b[38;5;241m=\u001b[39m (Grouping(object_refs\u001b[38;5;241m=\u001b[39m[ttp\u001b[38;5;241m.\u001b[39mid], context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeed Event\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py311\\Lib\\site-packages\\stix2\\datastore\\__init__.py:120\u001b[0m, in \u001b[0;36mDataStoreMixin.query\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve STIX objects matching a set of filters.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03mTranslate query() call to the appropriate DataSource call.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m has no data source to query\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py311\\Lib\\site-packages\\stix2\\datastore\\memory.py:358\u001b[0m, in \u001b[0;36mMemorySource.query\u001b[1;34m(self, query, _composite_filters)\u001b[0m\n\u001b[0;32m    351\u001b[0m all_objs \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m    352\u001b[0m     value\u001b[38;5;241m.\u001b[39mall_versions\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, _ObjectFamily)\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m [value]\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    355\u001b[0m )\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Apply STIX common property filters.\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m all_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(apply_common_filters(all_objs, query))\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_data\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py311\\Lib\\site-packages\\stix2\\datastore\\filters.py:131\u001b[0m, in \u001b[0;36mapply_common_filters\u001b[1;34m(stix_objs, query)\u001b[0m\n\u001b[0;32m    129\u001b[0m clean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filter_ \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[1;32m--> 131\u001b[0m     match \u001b[38;5;241m=\u001b[39m \u001b[43m_check_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilter_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstix_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[0;32m    134\u001b[0m         clean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py311\\Lib\\site-packages\\stix2\\datastore\\filters.py:171\u001b[0m, in \u001b[0;36m_check_filter\u001b[1;34m(filter_, stix_obj)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stix_obj[prop], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m stix_obj[prop]:\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_check_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\py311\\Lib\\site-packages\\stix2\\datastore\\filters.py:158\u001b[0m, in \u001b[0;36m_check_filter\u001b[1;34m(filter_, stix_obj)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# For properties like granular_markings and external_references\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# need to extract the first property from the string.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m prop \u001b[38;5;241m=\u001b[39m filter_\u001b[38;5;241m.\u001b[39mproperty\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prop \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstix_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# check filter \"property\" is in STIX object - if cant be\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# applied to STIX object, STIX object is discarded\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# (i.e. did not make it through the filter)\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m filter_\u001b[38;5;241m.\u001b[39mproperty:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# Check embedded properties, from e.g. granular_markings or external_references\u001b[39;00m\n",
      "File \u001b[1;32m<frozen _collections_abc>:788\u001b[0m, in \u001b[0;36mkeys\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32m<frozen _collections_abc>:812\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, mapping)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Phase 2: Make APT Threat Profiles\n",
    "attack = Attck()\n",
    "\n",
    "# Dictionary to hold all the threat profiles. \n",
    "# The key is a given APT and the values are a list of TTPs\n",
    "threat_profiles = {}\n",
    "\n",
    "for actor in attack.enterprise.actors:\n",
    "    technique_dict = {}\n",
    "    for technique in actor.techniques:\n",
    "        technique_dict[technique.id[0:5]] = 1\n",
    "    threat_profiles[actor.id] = technique_dict\n",
    "    \n",
    "print(threat_profiles[\"G0085\"])\n",
    "\n",
    "\n",
    "# TODO: Add extended_threat profiles\n",
    "# Create threatprofile Seed -- need to parse the value of the keys.\n",
    "extended_threat_profiles = {}\n",
    "for apt, t_codes in threat_profiles.items():\n",
    "    ttp_list = list(t_codes.keys())\n",
    "    profile, attackPatterns, relationships = CreateRelationships(ttp_list, aprioriList)\n",
    "    extended_threat_profiles[apt] = profile\n",
    "\n",
    "print(extended_threat_profiles[\"G0085\"])\n",
    "\n",
    "# TODO: Add progress bar??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: WEIGHTED JACCARD SIMILARITY\n",
    "# Side note: Find a way to relate techniques and sub-techniques. \n",
    "\n",
    "# TODO: Create a function that compares two threat profiles and produces a Weighted Jaccard. Just returns a number. \n",
    "def weighted_jac (profile1, profile2):\n",
    "\n",
    "    # Extract the keys as sets\n",
    "    keys1 = set(profile1.keys())\n",
    "    keys2 = set(profile2.keys())\n",
    "\n",
    "    # Calculate the Jaccard index\n",
    "    # Only need the normal intersection and union\n",
    "    intersection = keys1.intersection(keys2)\n",
    "    union = keys1.union(keys2)\n",
    "    # jaccard_index = len(intersection) / len(union)\n",
    "\n",
    "    # Weight the Jaccard index by the values in the dictionaries\n",
    "    weighted_intersection = sum(min(profile1[k], profile2[k]) for k in intersection)\n",
    "    weighted_union = sum(max(profile1.get(k, 0), profile2.get(k, 0)) for k in union)\n",
    "    weighted_jaccard_index = weighted_intersection / weighted_union\n",
    "    \n",
    "    return weighted_jaccard_index\n",
    "    #print(\"Weighted Jaccard index:\", weighted_jaccard_index)\n",
    "\n",
    "\n",
    "# AAG APT\n",
    "#p1 = {'T1566': 0.836, 'T1204': 0.944, 'T1059': 0.752, 'T1105': 0.74, 'T1027': 0.751, 'T1071': 0.778, 'T1082': 0.819, 'T1547': 0.745, 'T1140': 0.745, 'T1057': 0.753, 'T1083': 0.78, 'T1070': 0.7}\n",
    "# Known APT: G0085\n",
    "#p2 = {'T1114': 1, 'T1078': 1, 'T1566': 1, 'T1056': 1, 'T1204': 1, 'T1059': 1, 'T1564': 1, 'T1071': 1, 'T1090': 1}\n",
    "\n",
    "# weighted_jac(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create another function that calls the previous function on all of the threat profiles and our threat profile of interest.\n",
    "# We could do some sort of ranking like list the APTs with the highest Weighted Jaccard first. \n",
    "def compare_apt(h_threatprofile, threat_profiles):\n",
    "    # Make another dictionary \n",
    "    rankings = {}\n",
    "    \n",
    "    for key, value in threat_profiles.items(): \n",
    "        similarity  = (weighted_jac(value, h_threatprofile)) * 100\n",
    "        percentage = round(similarity, 2)\n",
    "        rankings[key] = percentage\n",
    "    \n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_apt_dict = compare_apt(apt_x, extended_threat_profiles)\n",
    "\n",
    "# SORTED -- NICE!\n",
    "sorted_dict = dict(sorted(compare_apt_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "print(sorted_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some sort of visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example dictionary\n",
    "data = sorted_dict\n",
    "\n",
    "# Extract keys and values from dictionary\n",
    "labels = list(data.keys())\n",
    "values = list(data.values())\n",
    "\n",
    "# Create a bar chart using matplotlib\n",
    "plt.bar(labels, values)\n",
    "\n",
    "# Set chart title and axis labels\n",
    "plt.title('APT THANGS')\n",
    "plt.xlabel('APTs')\n",
    "plt.ylabel('Similarity (%)')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dictionary to a JSON file\n",
    "import json\n",
    "\n",
    "with open(\"Extended_Threat_Profiles\", \"w\") as outfile:\n",
    "    json.dump(extended_threat_profiles, outfile)\n",
    "    \n",
    "# Ahhhhh purrrrrr (pian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                 | 1/124 [00:17<36:33, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                                | 2/124 [00:19<17:17,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▉                                                                                | 3/124 [00:20<10:20,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G0036\n"
     ]
    }
   ],
   "source": [
    "## Here we evaluate our models \n",
    "test_set = GenerateTestSet() # this is just G code and associated TTPs, need to make profiles\n",
    "test_profiles = {}\n",
    "\n",
    "# create profiles using the test data and make it into a dictionary\n",
    "# we filter data here to remove all empty lists\n",
    "for gcode, tcodes in tqdm(test_set.items()):\n",
    "    if (len(tcodes) < 1):\n",
    "        continue\n",
    "    tcodes = [tc[0:5] for tc in tcodes]\n",
    "    profile, attackPatterns, relationships = CreateRelationships(tcodes, aprioriList)\n",
    "    test_profiles[gcode] = profile\n",
    "    print(gcode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf4546b72c8f221321ebd81cfb6f77ad08fd1107692848c85b98477d65bf1073"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
